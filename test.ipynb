{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show boto3 pdfplumber chromadb langchain tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import io\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# AWS setup\n",
    "s3_client = boto3.client('s3')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'path/to/pdfs/'\n",
    "\n",
    "# Initialize Chroma\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=None))\n",
    "collection = chroma_client.create_collection(name=\"pdf_embeddings\")\n",
    "\n",
    "# Semantic text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=30000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Folder setup\n",
    "temp_folder = \"temp-pdf\"\n",
    "if os.path.exists(temp_folder):\n",
    "    shutil.rmtree(temp_folder)  # Delete folder and contents\n",
    "os.makedirs(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_titan_embedding(text, model_id=\"amazon.titan-embed-text-v2:0\", dimensions=1024, normalize=True):\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": text,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"normalize\": normalize\n",
    "        })\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept='application/json',\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embedding']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "def process_pdf(file_content, file_key, temp_folder):\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    file_key_safe = file_key.replace('/', '_')  # Replace invalid characters for filenames\n",
    "    with pdfplumber.open(io.BytesIO(file_content)) as pdf:\n",
    "        for page_num, page in enumerate(pdf):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if text:\n",
    "                text_chunks = text_splitter.split_text(text)\n",
    "                for idx, chunk in enumerate(text_chunks):\n",
    "                    chunks.append(chunk)\n",
    "                    meta = {\"file\": file_key, \"page\": page_num + 1, \"type\": \"text\"}\n",
    "                    metadata.append(meta)\n",
    "                    # Save chunk to file\n",
    "                    chunk_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_text_{idx}.txt\")\n",
    "                    with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{chunk}\")\n",
    "            tables = page.extract_tables()\n",
    "            for table_idx, table in enumerate(tables):\n",
    "                table_text = \"\\n\".join([\",\".join(row) for row in table if row])\n",
    "                if table_text:\n",
    "                    chunks.append(table_text[:40000])\n",
    "                    meta = {\"file\": file_key, \"page\": page_num + 1, \"type\": \"table\"}\n",
    "                    metadata.append(meta)\n",
    "                    # Save table to file\n",
    "                    table_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_table_{table_idx}.txt\")\n",
    "                    with open(table_file, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{table_text}\")\n",
    "            urls = extract_urls(text)\n",
    "            for url_idx, url in enumerate(urls):\n",
    "                chunks.append(url)\n",
    "                meta = {\"file\": file_key, \"page\": page_num + 1, \"type\": \"url\"}\n",
    "                metadata.append(meta)\n",
    "                # Save URL to file\n",
    "                url_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_url_{url_idx}.txt\")\n",
    "                with open(url_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{url}\")\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.pdf')]\n",
    "\n",
    "results = []\n",
    "for file_key in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    print(f\"Processing {file_key}...\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        file_content = obj['Body'].read()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading {file_key}: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        chunks, metadata = process_pdf(file_content, file_key, temp_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_key}: {e}\")\n",
    "        continue\n",
    "    embeddings = []\n",
    "    valid_chunks = []\n",
    "    valid_metadata = []\n",
    "    for chunk, meta in zip(chunks, metadata):\n",
    "        embedding = generate_titan_embedding(chunk)\n",
    "        if embedding:\n",
    "            embeddings.append(embedding)\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_metadata.append(meta)\n",
    "    if embeddings:\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=valid_chunks,\n",
    "            metadatas=valid_metadata,\n",
    "            ids=[f\"{file_key}_chunk_{i}\" for i in range(len(valid_chunks))]\n",
    "        )\n",
    "    results.append({\n",
    "        \"file\": file_key,\n",
    "        \"num_chunks\": len(valid_chunks),\n",
    "        \"num_embeddings\": len(embeddings),\n",
    "        \"saved_files\": [f for f in os.listdir(temp_folder) if f.startswith(file_key.replace('/', '_'))]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85aa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.pdf')]\n",
    "\n",
    "results = []\n",
    "for file_key in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    print(f\"Processing {file_key}...\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        file_content = obj['Body'].read()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading {file_key}: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        chunks, metadata = process_pdf(file_content, file_key, include_images=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_key}: {e}\")\n",
    "        continue\n",
    "    embeddings = []\n",
    "    valid_chunks = []\n",
    "    valid_metadata = []\n",
    "    for chunk, meta in zip(chunks, metadata):\n",
    "        embedding = generate_titan_embedding(chunk)\n",
    "        if embedding:\n",
    "            embeddings.append(embedding)\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_metadata.append(meta)\n",
    "    if embeddings:\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=valid_chunks,\n",
    "            metadatas=valid_metadata,\n",
    "            ids=[f\"{file_key}_chunk_{i}\" for i in range(len(valid_chunks))]\n",
    "        )\n",
    "    results.append({\n",
    "        \"file\": file_key,\n",
    "        \"num_chunks\": len(valid_chunks),\n",
    "        \"num_embeddings\": len(embeddings)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f93e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find information about machine learning\"\n",
    "query_embedding = generate_titan_embedding(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "\n",
    "query_results = []\n",
    "for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "    query_results.append({\n",
    "        \"file\": meta['file'],\n",
    "        \"page\": meta['page'],\n",
    "        \"type\": meta['type'],\n",
    "        \"distance\": dist,\n",
    "        \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "    })\n",
    "\n",
    "pd.DataFrame(query_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
