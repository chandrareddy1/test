{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 pdfplumber chromadb langchain pymupdf pytesseract pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import io\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# AWS setup\n",
    "s3_client = boto3.client('s3')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'path/to/pdfs/'  # Optional folder in bucket\n",
    "\n",
    "# Initialize Chroma\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=None))\n",
    "collection = chroma_client.create_collection(name=\"pdf_embeddings\")\n",
    "\n",
    "# Semantic text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=30000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_titan_embedding(text, model_id=\"amazon.titan-embed-text-v2:0\", dimensions=1024, normalize=True):\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": text,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"normalize\": normalize\n",
    "        })\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept='application/json',\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embedding']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "def process_pdf(file_content, file_key, include_images=False):\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    with pdfplumber.open(io.BytesIO(file_content)) as pdf:\n",
    "        for page_num, page in enumerate(pdf):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if text:\n",
    "                text_chunks = text_splitter.split_text(text)\n",
    "                for chunk in text_chunks:\n",
    "                    chunks.append(chunk)\n",
    "                    metadata.append({\"file\": file_key, \"page\": page_num + 1, \"type\": \"text\"})\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                table_text = \"\\n\".join([\",\".join(row) for row in table if row])\n",
    "                if table_text:\n",
    "                    chunks.append(table_text[:40000])\n",
    "                    metadata.append({\"file\": file_key, \"page\": page_num + 1, \"type\": \"table\"})\n",
    "            urls = extract_urls(text)\n",
    "            for url in urls:\n",
    "                chunks.append(url)\n",
    "                metadata.append({\"file\": file_key, \"page\": page_num + 1, \"type\": \"url\"})\n",
    "    if include_images:\n",
    "        doc = fitz.open(stream=file_content, filetype=\"pdf\")\n",
    "        for page_num, page in enumerate(doc):\n",
    "            images = page.get_images()\n",
    "            for img in images:\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image = Image.open(io.BytesIO(base_image[\"image\"]))\n",
    "                text = pytesseract.image_to_string(image)\n",
    "                if text.strip():\n",
    "                    image_chunks = text_splitter.split_text(text)\n",
    "                    for chunk in image_chunks:\n",
    "                        chunks.append(chunk)\n",
    "                        metadata.append({\"file\": file_key, \"page\": page_num + 1, \"type\": \"image\"})\n",
    "        doc.close()\n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85aa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.pdf')]\n",
    "\n",
    "results = []\n",
    "for file_key in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    print(f\"Processing {file_key}...\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        file_content = obj['Body'].read()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading {file_key}: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        chunks, metadata = process_pdf(file_content, file_key, include_images=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_key}: {e}\")\n",
    "        continue\n",
    "    embeddings = []\n",
    "    valid_chunks = []\n",
    "    valid_metadata = []\n",
    "    for chunk, meta in zip(chunks, metadata):\n",
    "        embedding = generate_titan_embedding(chunk)\n",
    "        if embedding:\n",
    "            embeddings.append(embedding)\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_metadata.append(meta)\n",
    "    if embeddings:\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=valid_chunks,\n",
    "            metadatas=valid_metadata,\n",
    "            ids=[f\"{file_key}_chunk_{i}\" for i in range(len(valid_chunks))]\n",
    "        )\n",
    "    results.append({\n",
    "        \"file\": file_key,\n",
    "        \"num_chunks\": len(valid_chunks),\n",
    "        \"num_embeddings\": len(embeddings)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f93e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find information about machine learning\"\n",
    "query_embedding = generate_titan_embedding(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "\n",
    "query_results = []\n",
    "for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "    query_results.append({\n",
    "        \"file\": meta['file'],\n",
    "        \"page\": meta['page'],\n",
    "        \"type\": meta['type'],\n",
    "        \"distance\": dist,\n",
    "        \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "    })\n",
    "\n",
    "pd.DataFrame(query_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
