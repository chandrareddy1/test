{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show boto3 pdfplumber chromadb langchain tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import io\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pdf_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# AWS setup\n",
    "s3_client = boto3.client('s3')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'path/to/pdfs/'\n",
    "\n",
    "# Initialize Chroma\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=None))\n",
    "collection = chroma_client.create_collection(name=\"pdf_embeddings\")\n",
    "\n",
    "# Semantic text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=30000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Folder setup\n",
    "temp_folder = \"temp-pdf\"\n",
    "if os.path.exists(temp_folder):\n",
    "    shutil.rmtree(temp_folder)  # Delete folder and contents\n",
    "os.makedirs(temp_folder)  # Recreate empty folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing non-UTF-8 characters and control characters.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)  # Convert non-strings (e.g., None, bytes) to string\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    # Remove control characters\n",
    "    text = ''.join(c for c in text if c.isprintable() or c.isspace())\n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def validate_chunk(text):\n",
    "    \"\"\"Validate chunk for Titan embedding input.\"\"\"\n",
    "    if not text:\n",
    "        return False, \"Empty or null input\"\n",
    "    if len(text) > 50000:  # Titan limit\n",
    "        return False, f\"Input too long ({len(text)} characters)\"\n",
    "    if not any(c.isalnum() for c in text):\n",
    "        return False, \"No alphanumeric content\"\n",
    "    return True, \"\"\n",
    "\n",
    "def generate_titan_embedding(text, model_id=\"amazon.titan-embed-text-v2:0\", dimensions=1024, normalize=True):\n",
    "    cleaned_text = clean_text(text)\n",
    "    is_valid, reason = validate_chunk(cleaned_text)\n",
    "    if not is_valid:\n",
    "        logger.warning(f\"Invalid input for embedding: {reason}\")\n",
    "        return None\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": cleaned_text,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"normalize\": normalize\n",
    "        })\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept='application/json',\n",
    "            contentType='application/json'\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embedding']\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "def is_valid_pdf(file_content):\n",
    "    \"\"\"Check if the content is a valid PDF.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(file_content)) as pdf:\n",
    "            if not pdf.pages or len(pdf.pages) == 0:\n",
    "                return False\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Invalid PDF detected: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_pdf(file_content, file_key, temp_folder):\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    file_key_safe = file_key.replace('/', '_')\n",
    "    \n",
    "    # Save raw PDF for debugging\n",
    "    raw_pdf_path = os.path.join(temp_folder, f\"{file_key_safe}_raw.pdf\")\n",
    "    with open(raw_pdf_path, 'wb') as f:\n",
    "        f.write(file_content)\n",
    "    logger.info(f\"Saved raw PDF: {raw_pdf_path}\")\n",
    "    \n",
    "    # Validate PDF\n",
    "    if not is_valid_pdf(file_content):\n",
    "        logger.error(f\"Skipping {file_key}: Invalid or empty PDF\")\n",
    "        return chunks, metadata\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(file_content)) as pdf:\n",
    "            if pdf.pages is None:\n",
    "                logger.error(f\"Skipping {file_key}: No pages found\")\n",
    "                return chunks, metadata\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if text:\n",
    "                    text_chunks = text_splitter.split_text(text)\n",
    "                    for idx, chunk in enumerate(text_chunks):\n",
    "                        cleaned_chunk = clean_text(chunk)\n",
    "                        if cleaned_chunk:\n",
    "                            chunks.append(cleaned_chunk)\n",
    "                            meta = {\"file\": file_key, \"page\": page_num + 1, \"type\": \"text\"}\n",
    "                            metadata.append(meta)\n",
    "                            chunk_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_text_{idx}.txt\")\n",
    "                            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                                f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{cleaned_chunk}\")\n",
    "                tables = page.extract_tables()\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    table_text = \"\\n\".join([\",\".join(str(cell) if cell is not None else \"\" for cell in row) for row in table if row])\n",
    "                    cleaned_table = clean_text(table_text)\n",
    "                    if cleaned_table:\n",
    "                        chunks.append(cleaned_table[:40000])\n",
    "                        meta = {\"file\": file_key, \"page\": page_num + 1, \"type\": \"table\"}\n",
    "                        metadata.append(meta)\n",
    "                        table_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_table_{table_idx}.txt\")\n",
    "                        with open(table_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{cleaned_table}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Skipped empty or invalid table at {file_key} page {page_num+1}\")\n",
    "                urls = extract_urls(text)\n",
    "                for url_idx, url in enumerate(urls):\n",
    "                    cleaned_url = clean_text(url)\n",
    "                    if cleaned_url:\n",
    "                        chunks.append(cleaned_url)\n",
    "                        meta = {\"file\": url, \"page\": page_num +  \"1_url\", \"type\": \"url\"}\n",
    "                        metadata.append(cleaned_url)\n",
    "                        url_file = os.path.join(temp_folder, f\"{file_key_safe}_page{page_num+1}_url_{url_idx}.txt\")\n",
    "                        with open(url_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent: {cleaned_url}\\n\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_key}: {e}\")\n",
    "        return chunks, metadata\n",
    "    \n",
    "    return chunks, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023780c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='')\n",
    "pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.pdf')]\n",
    "\n",
    "valid_pdfs = []\n",
    "for key in tqdm(pdf_files, desc=\"Validating PDFs\"):\n",
    "    try:\n",
    "        obj = s3_client.head_object(Bucket=bucket_name, Key=key)\n",
    "        if obj.get('ContentType') == 'application/pdf' and obj.get('ContentLength') > 1024:\n",
    "            valid_pdfs.append(key)\n",
    "        else:\n",
    "            logger.warning(f\"Skipping {key}: Invalid Content-Type ({obj.get('ContentType')}) or too small ({obj.get('ContentLength')} bytes)\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error checking {key}: {e}\")\n",
    "\n",
    "results = []\n",
    "for file_key in tqdm(valid_pdfs, desc=\"Processing PDFs\"):\n",
    "    logger.info(f\"Processing {file_key}...\"\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        file_content = obj['Body'].read()\n",
    "        if not file_content:\n",
    "            logger.warning(f\"Skipping {file_key}: Empty file\")\n",
    "            continue\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error downloading {file_key}: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        chunks, metadata = process_pdf(file_content, file_key, temp_folder)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing {file_key}: {e}\")\n",
    "        continue\n",
    "    embeddings = []\n",
    "    valid_chunks = []\n",
    "    valid_metadata = []\n",
    "    for chunk, meta in zip(chunks, metadata):\n",
    "        embedding = generate_titan_embedding(chunk)\n",
    "        if embedding:\n",
    "            embeddings.append(embedding)\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_metadata.append(meta)\n",
    "        else:\n",
    "            # Save invalid chunk for debugging\n",
    "            invalid_file = os.path.join(temp_folder, f\"{file_key.replace('/', '_')}_invalid_chunk.txt\")\n",
    "            with open(invalid_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Metadata: {json.dumps(meta)}\\n\\nContent:\\n{chunk}\\n\\n\")\n",
    "    if embeddings:\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=valid_chunks,\n",
    "            metadatas=valid_metadata,\n",
    "            ids=[f\"{file_key}_chunk_{i}\" for i in range(len(valid_chunks))]\n",
    "        )\n",
    "    results.append({\n",
    "        \"file\": file_key,\n",
    "        \"num_chunks\": len(valid_chunks),\n",
    "        \"num_embeddings\": len(embeddings),\n",
    "        \"saved_files\": [f for f in os.listdir(temp_folder) if f.startswith(file_key.replace('/', '_'))]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find information about machine learning\"\n",
    "query_embedding = generate_titan_embedding(query)\n",
    "results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "\n",
    "query_results = []\n",
    "for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "    query_results.append({\n",
    "        \"file\": meta['file'],\n",
    "        \"page\": meta['page'],\n",
    "        \"type\": meta['type'],\n",
    "        \"distance\": dist,\n",
    "        \"content\": doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "    })\n",
    "\n",
    "pd.DataFrame(query_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
